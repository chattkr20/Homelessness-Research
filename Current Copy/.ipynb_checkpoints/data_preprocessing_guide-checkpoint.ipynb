{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3S6Ck3TLwbk"
   },
   "source": [
    "# ðŸ‘‹ **Welcome to the numerical data preprocessing notebook!**\n",
    "In this notebook, we'll look at a variety of data cleaning, transformation, and reduction techniques that compose the intricate process of data preprocessing. We will learn what steps we can take to get the most out of our data and generate data that will be most useful and impactful when it comes to creating machine learning models. This notebook will act as a general guide in working with numerical data.\n",
    "\n",
    "Language and Image data require additional preprocessing to generate some form of numerical data. For this reason, we begin the journey of data preprocessing with the most fundamental numerical data and will move onto the more advanced data types in the coming weeks. The techniques and exercises in this notebook are applicable to any **numerical data**.\n",
    "\n",
    "In this notebook, we'll be working with medical data that tries to answer the question: **Can we predict medical costs given patient information?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pF7-0_D1ANpw"
   },
   "outputs": [],
   "source": [
    "#@title Loading in our data and importing necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Our data is stored on Google Cloud.\n",
    "# We can use a command like this to upload it into Google Colab's file system\n",
    "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20%2B%20X/Independent/Main%20Curriculum/Numerical%20Data%20Preprocessing/medical_costs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYrvRG9j_CLi"
   },
   "outputs": [],
   "source": [
    "# Let's load in our dataset as a pandas dataframe\n",
    "medical_data = pd.read_csv(\"medical_costs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIhxNU0KQM9O"
   },
   "source": [
    "# ðŸ› ï¸ **Standard Tools to Review**\n",
    "\n",
    "Here are some functions that you've likely seen before that will come in handy while working on your project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xi0iOvqRipk"
   },
   "source": [
    "## ðŸ”¢ **Numpy functions**\n",
    "### **Using `.shape`:**\n",
    "We can use `.shape` to see the shape of any numpy array by doing\n",
    "\n",
    "```python\n",
    "ARRAY_NAME.shape or np.shape(ARRAY_NAME)\n",
    "```\n",
    "[See the documentation here](https://numpy.org/doc/stable/reference/generated/numpy.shape.html).\n",
    "\n",
    "### **Using `np.where`**\n",
    "This function can be used to find the indices of an array where the value equals what you're checking. See the general and specific example below.\n",
    "```python\n",
    "np.where(ARRAY == VALUE) # general\n",
    "np.where(ARRAY == 15) # will return indices where the array = 15\n",
    "```\n",
    "[See the documentation here](https://numpy.org/doc/stable/reference/generated/numpy.where.html).\n",
    "\n",
    "### **Using `np.min` and `np.max`**\n",
    "We can use these functions to find either the minimum or maximum value in an array.\n",
    "```python\n",
    "np.min(ARRAY) or np.max(ARRAY)\n",
    "```\n",
    "See the documentation [here for min (equivalent to amin)](https://numpy.org/doc/stable/reference/generated/numpy.amin.html#numpy.amin) and [here for max (equivalent to amax)](https://numpy.org/doc/stable/reference/generated/numpy.amax.html#numpy.amax).\n",
    "\n",
    "### **Using `np.sum` and `np.average`**\n",
    "We can use these functions to find either the sum or average of an array.\n",
    "```python\n",
    "np.sum(ARRAY) or np.average(ARRAY)\n",
    "```\n",
    "See the documentation [here for sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) and [here for average](https://numpy.org/doc/stable/reference/generated/numpy.average.html).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNj5t-7VWHoX"
   },
   "source": [
    "## ðŸ“ˆ **Plotting**\n",
    "We've seen a number of plotting functions in seaborn, but here are just a few!\n",
    "* `sns.countplot(ARRAY)` to plot the number of each element in an array\n",
    "* `sns.histplot(ARRAY)` to plot a histogram of the values in that array\n",
    "* `sns.scatterplot(x = X_FEATURE, y = Y_FEATURE, data = DATASET)` to make a scatterplot between two features\n",
    "* `sns.catplot(x = X_FEATURE, y = Y_FEATURE, data = DATASET)` to make a categorical plot between two features\n",
    "* `sns.barplot(x = X_DATA, y = Y_DATA)` to make a bar plot between two features\n",
    "\n",
    "Check out the [seaborn API reference](https://seaborn.pydata.org/api.html) for more options.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QoQfa8Mm5yv"
   },
   "source": [
    "## ðŸ¼ **Pandas**\n",
    "Here's a list of some useful Pandas tools and commands. To check out more Pandas functionality, take a look at [the documentation here](https://pandas.pydata.org/docs/reference/index.html#api).\n",
    "\n",
    "* `len(NAME_OF_DATA)` to see how many rows are in a dataframe\n",
    "* `NAME_OF_DATA[COLUMN_NAME]` to index into a column in a data frame\n",
    "* `NAME_OF_DATA[[COLUMN1_NAME, COLUMN2_NAME...]]` to index into multiple columns\n",
    "* `NAME_OF_DATA[COLUMN_NAME].value_counts()` to count the number of values in each category in a particular column  \n",
    "* `NAME_OF_DATA[COLUMN_NAME].unique()` to see all unique values in a column\n",
    "* `NAME_OF_DATA.head()` to get the first 5 rows in a dataframe\n",
    "* `NAME_OF_DATA.tail()` to get the last 5 rows in a dataframe\n",
    "* `NAME_OF_DATA.sort_values(by=COLUMN_NAME)` to sort the dataframe from least to greatest value according to the column chosen\n",
    "* `NAME_OF_DATA.iloc[ROW_NUMBER]` to get a specific row in a dataframe\n",
    "* `NAME_OF_DATA.isna()` to count the number of null values by column\n",
    "* `NAME_OF_DATA.fillna()` to fill the null values in your dataframe\n",
    "\n",
    "You can grab values in a column based on a specific condition as well! Let's say you wanted to look at all of the patients that have more than 2 children in their household. You could do this by writing:\n",
    "\n",
    "```python\n",
    "medical_data[medical_data['children']>2]\n",
    "```\n",
    "Or, let's say you wanted to all the patients with exactly 5 children. You could do this like:\n",
    "```python\n",
    "medical_data[medical_data['children']==5]\n",
    "```\n",
    "\n",
    "You can combine this idea with the operations above to do some pretty cool stuff!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81-0X31vbmDT"
   },
   "source": [
    "# ðŸ« **Let's learn more about the data!**\n",
    "\n",
    "We're going to use the functions above to answer some questions about the data set. Work together with your coding group to come up with the solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6a8L8M1Iiqb"
   },
   "source": [
    "Every time when you're starting out with a new data set, there's one question you need to ask yourself first: is the fundamental question this data can answer a **regression** problem or a **classification** problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "930nXeB_I9QU"
   },
   "outputs": [],
   "source": [
    "#@title Regression or Classification?\n",
    "r_or_c = \"\" #@param [\"\", \"Regression\", \"Classification\"]\n",
    "\n",
    "if r_or_c == \"Regression\":\n",
    "  print(\"Correct! Regression problems deal with numerical predictions.\")\n",
    "else:\n",
    "  print(\"Incorrect. Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qycxd2lgxEvD"
   },
   "source": [
    "## **To start out, let's look at some of the data set so we can see what some data points look like!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBpIvI16xEvD"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJaRVRUaaJ3U"
   },
   "source": [
    "## **What are the dimensions of the data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqjyEyvpZ_wC"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPPVhBYHbxd3"
   },
   "source": [
    "## **What are the relevant columns in this data set?**\n",
    "In this data set, we have a number of different columns - some of which may be more useful than others. One column will serve as the output, and the rest will serve as the input. Which of the data columns can serve as the output (`y`) to our model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "MbuWt7hjbxd3"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#@markdown Our output should be:\n",
    "data_output = \"\" #@param [\"\", \"age\", \"sex\", \"bmi\", \"children\", \"smoker\", \"region\", \"charges\"]\n",
    "\n",
    "if data_output == 'charges':\n",
    "  print(\"That's the correct output!\")\n",
    "else:\n",
    "  print(\"That's not the correct output, try again!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gS9vBiednOv"
   },
   "source": [
    "## **What does the distribution of our data set look like based on medical charges?**\n",
    "To answer this question, you want to plot the distribution of views in the data set!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHpmCOjBd9y8"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLIOliW0awhF"
   },
   "source": [
    "Some follow up questions to consider:\n",
    "* Are there any outliers in the data set?\n",
    "* If there are outliers, is it more useful to keep them in or get rid of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6FPxOkFkhg_"
   },
   "source": [
    "## **What is the range of the output in our data set (i.e. the minimum and maximum)?**\n",
    "\n",
    "This information will be useful to know for a regression problem!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yqY2xy6khhL"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVflZ59-ri5p"
   },
   "source": [
    "## **What is the sum and average cost of medical expenses for all the patients?**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilogSqPPri51"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx6tu5L78J-K"
   },
   "source": [
    "## **Check out the smoking status of the 5 patients with the highest medical expenses and the 5 patients with the lowest medical expenses. Do you notice any relation between smoking status and the cost of medical expenses?**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJ0_y5qW8J-X"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnFEeyF9i5Oi"
   },
   "source": [
    "# âš™ï¸ **Pre-Processing Steps**\n",
    "Now that we've done a preliminary exploration of our dataset, it is time to delve deep and process the data to get it ready for model development. There are three overarching steps in pre-processing data that all consist of a variety of distinct techniques. These three steps are often referred to as:\n",
    "1. Cleaning\n",
    "2. Transformation\n",
    "3. Reduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmL2ui_9tD3P"
   },
   "source": [
    "## ðŸ§¹ **Cleaning your dataset**\n",
    "\n",
    "Often times, your data will not be perfect. You may be missing some values for specific features, there may be strange outliers that wildly skew the data, or there may be other sources of noise that have perturbed the existing values that you need to account for. While you could just throw out the data samples that don't beneficially contribute to your machine learning model, you may miss out on a lot of valuable information. So, there are many techniques to clean your data to minimize some of these errors.\n",
    "\n",
    "Let's take a look at **Data Imputation**, or the process of dealing with missing values in your dataset. Specifically, this process consists of filling in the missing values with estimates of the actual values.\n",
    "\n",
    "First, let's try to observe the problem at hand by solving the question below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gw2UrXDVDY0F"
   },
   "source": [
    "### **How many values in your dataset are missing (are equal to null)?**\n",
    "\n",
    "*Hint: Check out the pandas function [isnull()](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) in combination with a sum over the columns of the resulting dataframe.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCiLtPefDQsc"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4T6wN00F6RJ"
   },
   "source": [
    "### **Mean imputation of the missing values**\n",
    "\n",
    "As you have shown, there are many values in the **bmi** and **children** feature columns that are missing. Instead of getting rid of these data samples entirely, we will impute the data in these columns. A common way to perform this imputation is to fill in the missing values with the average value of the respective column. This way, these samples won't impact the machine learning model significantly one way or another and we can still benefit from the data.\n",
    "\n",
    "For this problem, replace the null values in your original dataframe with the average of the respective column. That is, replace the null values in the **bmi** column with the average of the **bmi** column and replace the null values in the **children** column with the average of the **children** column. Make sure to modify the original dataframe and not to create a new one.\n",
    "\n",
    "*Hint: Check out the pandas function [fillna()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) to simplify this process for each column.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kklFsEtJKRW"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcLvE6OjMugO"
   },
   "source": [
    "## ðŸ¦‹ **Transforming your dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITM9Kefa3fpO"
   },
   "source": [
    "### **Discretization**\n",
    "\n",
    "Machine learning models are stronger and more powerful with clean and meaningful data. You will transform the features in your dataset to be more meaningful and beneficial in a number of ways. Namely, you will convert the **age** feature to be one that is more categorical in nature - separating age into three separate bins:\n",
    "- young: 18-33 years old\n",
    "- middle: 34-49 years old\n",
    "- elder: 50+ years old\n",
    "\n",
    "Knowing the exact age of a patient is not the most interesting piece of information for our purposes. Instead, if we have general bins like the ones above, we can derive a relationship between the age range and the medical costs. In the function below, you will loop through all the rows of your dataframe and access the age field. For each age, append the string `'young'`, `'middle'`, or `'elder'` to the `age_category_features` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCuZZIeeRca7"
   },
   "outputs": [],
   "source": [
    "def get_categorical_age():\n",
    "  age_categorical_features = []\n",
    "  for i, row in medical_data.iterrows():\n",
    "    age = row['age']\n",
    "    ### BEGIN YOUR CODE HERE ###\n",
    "\n",
    "    ### END YOUR CODE HERE ###\n",
    "  return age_categorical_features\n",
    "\n",
    "age_categorical_features = get_categorical_age()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Jjuj3JJS30i"
   },
   "source": [
    "### **Numerical Transformation**\n",
    "\n",
    "Additionally, you will convert categorical variables like **sex**, **region**, **smoker**, and now **age** into their quantitative counterparts. ML models are very capable of dealing with quantities instead of strings with categorical features. For example, instead of working with the strings `'male'` and `'female'`, we will convert them into the value `0` and `1` respectively, where `0` represents `'male'` and `1` represents '`female'`.\n",
    "\n",
    "The transformations that we would like to make for each of the categorical features are defined below.\n",
    "\n",
    "**Sex**\n",
    "*   `'female'` âžœ `0`\n",
    "*   `'male'` âžœ `1`\n",
    "\n",
    "**Smoker**\n",
    "*   `'no'` âžœ `0`\n",
    "*   `'yes'` âžœ `1`\n",
    "\n",
    "**Region**\n",
    "*   `'northeast'` âžœ `0`\n",
    "*   `'northwest'` âžœ `1`\n",
    "*   `'southeast'` âžœ `2`\n",
    "*   `'southwest'` âžœ `3`\n",
    "\n",
    "**Age**\n",
    "*   `'elder'` âžœ `0`\n",
    "*   `'middle'` âžœ `1`\n",
    "*   `'young'` âžœ `2`\n",
    "\n",
    "\n",
    "In this problem, we will be converting all the categorical features to their respective quantities as defined above. We will be using a useful object known as a `LabelEncoder`. To use the `LabelEncoder`, we first import it from the `sklearn.preprocessing` package. Next, you initialize the `LabelEncoder` object as follows:\n",
    "\n",
    "```python\n",
    "le = LabelEncoder()\n",
    "```\n",
    "\n",
    "The process of converting your features requires two steps: *fitting* the LabelEncoder on the categorical features and then *transforming* your features from categorical to numerical. These two steps can be done as follows:\n",
    "\n",
    "```python\n",
    "le.fit(categorical_features)\n",
    "numerical_features = le.transform(categorical_features)\n",
    "```\n",
    "\n",
    "Consider this simple example below for age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4ffdonWZ2mE"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "example_categorical_features = ['young', 'young', 'middle', 'young', 'elder', 'middle']\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(example_categorical_features)\n",
    "example_numerical_features = le.transform(example_categorical_features)\n",
    "print(example_numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTd9rTYrz7oL"
   },
   "source": [
    "**Now it's your turn!** Write code below to encode the categorical features for **age**, **sex**, and **region**. Use the `age_category_features` list that you previously created as the input to the LabelEncoder for **age**. As for **sex**, **smoker**, and **region**, access the relevant columns from your `medical_data` pandas dataframe and use those as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvK1U-AF0t7U"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "### BEGIN YOUR CODE HERE\n",
    "\n",
    "############### Age ###############\n",
    "# STEP 1: Fit LabelEncoder on age_categorical_features\n",
    "# STEP 2: Transform age_categorical_features and save in variable named age_numerical_features\n",
    "\n",
    "############### Sex ###############\n",
    "# STEP 1: Fit LabelEncoder on sex_categorical_features\n",
    "# STEP 2: Transform sex_categorical_features and save in variable named sex_numerical_features\n",
    "\n",
    "############## Smoker #############\n",
    "# STEP 1: Fit LabelEncoder on smoker_categorical_features\n",
    "# STEP 2: Transform smoker_categorical_features and save in variable named smoker_numerical_features\n",
    "\n",
    "############## Region #############\n",
    "# STEP 1: Fit LabelEncoder on region_categorical_features\n",
    "# STEP 2: Transform region_categorical_features and save in variable named region_numerical_features\n",
    "\n",
    "### END YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ichg8VT3mMF"
   },
   "source": [
    "## â™Ÿï¸ **Reducing your datset**\n",
    "\n",
    "Not all features are equally important. In this data for example, the **smoker** status of a patient may be a far more impactful feature in determining medical costs than a patient's **region**. Likewise, some combination of the input features may be most impactful in distinguishing the patients.\n",
    "\n",
    "Many techniques have been created to *select* features or *reduce* features. We will be taking a look at a very common approach used to *reduce* the number of features from our input dataset. This technique is most useful when dealing with datasets with a large number of features (>1000 features) per sample.\n",
    "\n",
    "Before we attempt this approach, let us first download and read in the fully preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dUWj8csK4EKr"
   },
   "outputs": [],
   "source": [
    "#@title Load Preprocessed Dataset\n",
    "\n",
    "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20%2B%20X/Independent/Main%20Curriculum/Numerical%20Data%20Preprocessing/medical_costs_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zx-Ks8kr7MiJ"
   },
   "outputs": [],
   "source": [
    "# Let's load in our preprocessed dataset as a pandas dataframe\n",
    "medical_data_preprocessed = pd.read_csv('medical_costs_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Dn8dzyy8YDk"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 samples to observe the preprocessed and numerically transformed features\n",
    "medical_data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgVoVWXcB3hQ"
   },
   "source": [
    "We will be using a technique known as **Principal Component Analysis (PCA)** to create *2* new features that explain our dataset best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp1-pqEr8Kxe"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# (1) Initialize PCA object with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# (2) Learn Principal Components\n",
    "principal_components = pca.fit_transform(medical_data_preprocessed)\n",
    "\n",
    "# (3) Create Dataframe to hold spectograms and respective resistant antibiotics\n",
    "principal_df = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uMxlQsIMBkud"
   },
   "outputs": [],
   "source": [
    "#@title Run to Intitialize Plot Function\n",
    "\n",
    "def create_pca_plot(category):\n",
    "  principal_df[category] = medical_data_preprocessed[category] # Categories to visualize\n",
    "\n",
    "  fig = plt.figure(figsize = (12, 8))\n",
    "  ax = fig.add_subplot(1,1,1)\n",
    "  ax.set_xlabel('First Principal Component', fontsize = 15)\n",
    "  ax.set_ylabel('Second Principal Component', fontsize = 15)\n",
    "  targets = pd.unique(medical_data_preprocessed[category]).tolist() # All possible y values\n",
    "  colors = ['red', 'blue', 'orange', 'purple'] # different colors for different targets\n",
    "  colors = colors[:len(targets)]\n",
    "  for target, color in zip(targets, colors): # label points by antibiotic resistant target\n",
    "      indicesToKeep = principal_df[category] == target\n",
    "      ax.scatter(principal_df.loc[indicesToKeep, 'PC1'],\n",
    "                principal_df.loc[indicesToKeep, 'PC2'],\n",
    "                c = color, s = 40, alpha=0.75)\n",
    "  if category == 'smoker':\n",
    "    names = ['Non-Smoker', 'Smoker']\n",
    "  elif category == 'sex':\n",
    "    names = ['Female', 'Male']\n",
    "  elif category == 'region':\n",
    "    names = ['Northeast', 'Northwest', 'Southeast', 'Southwest']\n",
    "  ax.legend(targets, loc='lower right')\n",
    "  ax.get_xaxis().set_ticks([]);\n",
    "  ax.get_yaxis().set_ticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2Zvy2BqErx7"
   },
   "source": [
    "Running the cell above defines the plot function `create_pca_plot(category)` in which you can pass in the column category from the dataset and observe your results. Run the cell below to `create_pca_plot('smoker')` to observe a visualization of the PCA features when grouped by smoker status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcqYtTwIFF3t"
   },
   "outputs": [],
   "source": [
    "create_pca_plot('smoker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xtk_5ctHC_i9"
   },
   "source": [
    "The plot above is a visualization of the two new features, or components, you created with PCA when categorized by smoker status. As you can see, the samples are quite grouped together with other samples of similar smoker status. When you see clearly defined groups in your PCA plot, it signifies the value of the decomposition for the task at hand.\n",
    "\n",
    "**Now it's your turn!** Use the predefined `create_pca_plot(category)` function and pass in `'sex'`, `'region'`, or `'age'` for `category` and observe your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wkc4WJ7KEgEv"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJKq6Ig_Ldrh"
   },
   "source": [
    "<font color=\"#6495ED\"><h1><b>That's it for numerical data pre-processing!</b></h1></font>\n",
    "<h4>Like we said earlier, there are a number of ways to process data before sending it to your model, and here we've just covered some. Hopefully this will help you know how to begin your project code! </h4>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
